<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=/css/main.css media=all><title>braveheat.xyz | 向量检索算法HNSW</title></head><body><div id=container><header class=site-header><h1 class=site-title><a href=/>braveheat.xyz</a></h1></header><hr class=site-header-bottom><div id=content><div class=main role=main><article role=article class=article itemscope itemtype=http://schema.org/BlogPosting><header class=header><h1 class=title itemprop="name headline">向量检索算法HNSW</h1><span class=meta><time class=date datetime=2022-06-01T15:57:59+07:00 itemprop=datePublished>2022/06/01</time></span></header><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script><p>Hierarchical Navigable Small World (HNSW)是应用最广泛的近似最近邻检索算法之一，它基于小世界模型，采用类似跳表的分层索引机制，在保证查询效率的前提下，还能动态插入数据。HNSW 算法的原作者开源了一个参考实现<a href=https://github.com/nmslib/hnswlib>hnswlib</a>，本文将以此实现为基础对该算法进行讨论。</p><h2 id=索引结构>索引结构</h2><p>下图是 HNSW 索引结构的示意图（这张图出自 HNSW 的论文<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，在网上搜到的很多关于 HNSW 的资料都直接引用了这张图）。HNSW 索引是一个分层结构，每一层是一个邻近图（Proximity Graph），每个点有若干条无向边连接到它在该层内的近似最近邻。最底层（0 层）包括了所有数据点，从 1 层往上数据点的数量是指数递减的，第 <em>i+1</em> 层的点是从第 <em>i</em> 层随机选择的。最顶层只有一个点，作为查询的入口。</p><p><img src=/figures/hnsw.png alt=hnsw title="HNSW 索引结构"></p><p>在创建索引时，通过参数 M 来指定每一层中数据点的最大邻居数。对于第 0 层，\(M_{max0} = 2 * M\)，对于其他层，\(M_{max} = M\)。因此 M 越大，每个点占用的空间也就越大。</p><p>每个数据点投影到最大层数 \(l\)，是根据指数衰减概率计算出来的，公式为：\(l = -\log(uniform(0 .. 1)) * M_L\) （公式1）。其中 \(uniform(0 .. 1)\) 为区间 [0, 1) 上的均匀分布，\(M_L\) 是正则化因子，它决定了数据点被投影到 1 层及以上的概率。作者在论文中给出建议取值是 \(M_L = \dfrac{1}{\log M}\)。因此 \(M\) 越大，\(M_L\)越小，数据点被投影到 1 层以上的概率也就越小。</p><p>我估算了一下，发现通常数据点被提升的概率是很小的。假设 \(M = 16\)，那么 \(M_L \approx 1.2\)，从 0 层提升到一层的概率大概是 6% 左右。因此HNSW索引层高的期望值并不高，1 层以上的数据量也比较小，主要的空间开销在 0 层。但是既然是随机的，那么如果 \(uniform(0 .. 1)\) 的值无限趋近于 0，\(l\) 就会趋近于无穷大，所以理论上还是有可能产生出一个非常高的层级。</p><p>综合以上 2 点，参数 M 直接决定了索引占用空间的大小。虽然 M 越大，数据点被提升到 1 层及以上的概率越小，但是每个点在 0 层占用的空间却更大，而后者才是索引的主要空间开销，因此可以近似的认为索引的空间占用跟 M 是成正比的。这里的索引空间只是存储 HNSW 图结构的空间，不包括向量数据本身。在实践中，向量数据本身占用的空间经常是远大于索引的。比如 128 维 float 类型的向量，大小为 1KB。M = 16 的情况下，索引 0 层占用的空间为 2 * 16 * 4 = 128 字节，要比向量本身小多了。</p><h2 id=索引查询>索引查询</h2><p>在 HNSW 的分层结构中，第 1 到 <em>maxLevel</em> 层相当于为 0 层的全量数据建立了不同精度的缩略图，通过对这些缩略图的搜索，可以快速定位到 0 层中一个距离查询向量较近的点，从而缩小 0 层的搜索范围，达到提升查询效率的目的。</p><p>假设要查询向量 <em>q</em> 的 <em>k</em> 个最近邻居，HNSW 的搜索过程如下：</p><ol><li><p>从顶层入口点 <em>EP</em> 自上向下搜索到 1 层，每层找到距离 <em>q</em> 最近的 1 个点作为下一层的入口。层内搜索采用贪婪算法，具体来说，算法从当前层的入口点开始，遍历它的所有邻居，并计算每个邻居和 <em>q</em> 的距离，找到距离最小的邻居，移动到该点，并重复以上的搜索过程。如果所邻居点到 <em>q</em> 的距离都要大于当前点，则算法终止，当前点就是本层中距离 <em>q</em> 最近的点，也是下一层搜索的入口。</p></li><li><p>在 0 层用贪婪算法查找距离 <em>q</em> 最近的 <em>efSearch</em> 个点。同样使用贪婪算法，搜索过程使用一个小顶堆 <em>candidates</em> 作为搜索候选队列，使用一个最大容量为 <em>efSearch</em> 的大顶堆 <em>top_candidates</em> 作为结果候选队列。具体搜索方法为：从上一步中得到的 0 层入口点开始，遍历它的所有邻居，计算它们和 <em>q</em> 的距离 <em>d</em>，如果 <em>d</em> 比当前 <em>top_candidates</em> 中的最大距离要小，或者 <em>top_candidates</em> 还没有达到最大容量，就把该邻居放入 <em>top_candidates</em> 和 <em>candidates</em>，否则就丢弃。遍历完当前点的邻居之后，再从 <em>candidates</em> 中取出下一个点进行遍历，如果 <em>candidates</em> 为空，则搜索终止，从 <em>top_candidates</em> 中选出距离 <em>q</em> 最近的 <em>k</em> 个点作为结果返回。由于 HNSW 使用的是无向图，遍历过程中会出现环路，需要记录下已经访问过的每个点，如果再次遇到就直接跳过。</p></li></ol><h2 id=索引构建>索引构建</h2><p>索引构建的过程就是向一个空的索引中逐个插入数据点的过程。插入数据点 <em>q</em> 的步骤如下：</p><ol><li><p>按照公式 1 计算 <em>q</em> 最高投影的层数 <em>l</em> 。</p></li><li><p>从顶层入口点 <em>EP</em> 开始向下搜索到 <em>l+1</em> 层，每层找到 1 个距离 (q) 最近的点作为下一层的入口。方法跟查询步骤 1 相同。</p></li><li><p>从 <em>l</em> 层向下到 0 层，依次将 <em>q</em> 插入每一层，第 <em>i</em> 层插入过程如下：</p><p>a. 找到 <em>q</em> 在第 <em>i</em> 层的 \(M\_{max}(i)\) 个最近邻居，方法跟查询步骤 2 相同。</p><p>b. 建立 <em>q</em> 到这些邻居的边。</p><p>c. 对每个邻居点 <em>n</em> ，建立其到 <em>q</em> 的边。如果它的当前边数小于最大值 \(M_{max}(i)\) 则直接添加到 <em>q</em> 的边，否则就需要从 <em>n</em> 现有的邻居和 <em>q</em> 这 \(M_{max}(i)+1\) 个点中间选择 \(M_{max}(i)\) 个作为 <em>n</em> 的新邻居。<a href=https://github.com/nmslib/hnswlib/blob/master/hnswlib/hnswalg.h#L328>这里</a>的选择逻辑稍微有点绕，简单来说，就是按照到 <em>q</em> 的距离从小到大的顺序，逐个把所有候选点添加到 <em>n</em> 的邻接表，而每个添加的点 <em>x</em> 必须要满足如下条件：<em>x</em> 到 <em>q</em> 的距离要小于 <em>x</em> 到所有已经添加的点的距离。这个算法在原论文的第 3、4 节里都有说明，结合起来看更容易理解。不过，我暂时还没有理解为什么要用这种方式来选择。</p></li></ol><h2 id=删除和更新>删除和更新</h2><p>HNSW 算法的一大优势，就是可以动态插入数据，因为索引的构建过程本身就是逐条插入数据的过程。但是删除就不那么直接了。删除一个点时，也要同时调整它在每一层的邻居点，以保证邻近图的结构和性质不变，否则查询效率会受到影响，极端情况下甚至有可能变成非连通图，导致某些点在查询中无法被访问到。调整邻居点的最简单的做法就是把每个邻居点重新插入一遍，但是这样效率就太低了。</p><p>hnswlib 中的实现是通过标记的方式进行逻辑删除。实现简单，效率也很高，但是有两个问题：一是逻辑删除并不会释放空间，二是会影响后续查询的效率。少量删除时问题不大，但是如果删除的数据多了，这两个问题就会比较严重。</p><p>对于更新问题，一种简单的方案是把它看做先删除旧向量，然后添加新向量两个操作。这种方式下还是会有前面删除的问题。hnswlib 采用了另一种方案，直接覆盖旧的向量值，并且从 0 层向上逐层修改图结构。修改的方法跟插入不太一样，并不会用贪婪算法去遍历查找新点的最近邻居，而从旧点的 2 度邻居里查找。另外，算法会按照一定概率随机选择旧点的邻居进行更新，通过调整概率的大小，可以在性能和准确性之间进行平衡。hnswlib 的实现比较简单，很多问题并没有考虑周全，在并发负载下，这里更新算法的实现还是有一些问题。</p><h2 id=一些简单的测试>一些简单的测试</h2><p>hnswlib 中自带了一个简单的 benchmark 工具 sift_1b.cpp，使用 sift_1b 测试数据集对索引的构建和查询性能进行测试。运行测试前需要先执行 download_bigann.py 脚本下载测试数据集，数据集比较大，压缩包由 90 多 G，解药后 120 多 GB。测试数据集为 128 维向量，每个分量是 1 字节整数，向量总数量 10 亿。另外包含了一些测试向量，每个测试向量都预先计算好的最近邻居，用于召回率测试。</p><p>测试平台为阿里云<a href=https://help.aliyun.com/document_detail/25378.html#g6>ecs.g6.4xlarge</a>，16 核 64G 内存。</p><h3 id=构建速度>构建速度</h3><p>sift_1b.cpp 中使用了 openmp，默认并行线程数 = cpu 核心数，所以 cpu 基本上是全部跑满的。</p><table><thead><tr><th>数据集大小</th><th>索引参数</th><th>构建耗时(s)</th><th>平均速度(/s)</th></tr></thead><tbody><tr><td>10M</td><td>M = 16; efConstruct = 500</td><td>1327.72</td><td>7531</td></tr><tr><td>10M</td><td>M = 16; efConstruct = 100</td><td>323.895</td><td>30874</td></tr><tr><td>10M</td><td>M = 8; efConstruct = 100</td><td>214.525</td><td>46614</td></tr><tr><td>100M</td><td>M = 16; efConstruct = 40</td><td>1806.32</td><td>55361</td></tr></tbody></table><h3 id=内存占用>内存占用</h3><p>内存占用以测试结束时进程 RSS 为准。</p><table><thead><tr><th>数据集大小</th><th>索引参数</th><th>内存占用(MB)</th><th>持久化文件大小(GB)</th></tr></thead><tbody><tr><td>10M</td><td>M = 16; efConstruct = 500</td><td>4067</td><td>2.6</td></tr><tr><td>10M</td><td>M = 16; efConstruct = 100</td><td>4067</td><td>2.6</td></tr><tr><td>10M</td><td>M = 8; efConstruct = 100</td><td>3440</td><td>2.0</td></tr><tr><td>100M</td><td>M = 16; efConstruct = 40</td><td>40011</td><td>26</td></tr></tbody></table><h3 id=查询性能>查询性能</h3><p>查询性能主要是看召回率和耗时的关系。在 HNSW 算法中，增大参数 efSearch 可以提高结果的召回率，同时也会增加查询耗时。在下面的结果曲线中，横轴为召回率，纵轴为查询耗时，如果要对比不同算法，则曲线越靠近右下角表示算法效果越好。这里的结果都是<strong>单线程</strong>查询时的结果。</p><p>数据集 10M; M = 16; efConstruct = 500</p><p><img src=/figures/hnsw_perf_1.png alt=hnsw></p><p>数据集 10M; M = 16; efConstruct = 100</p><p><img src=/figures/hnsw_perf_2.png alt=hnsw></p><p>数据集 10M; M = 8; efConstruct = 100</p><p><img src=/figures/hnsw_perf_3.png alt=hnsw></p><p>数据集 100M; M = 16; efConstruct = 40</p><p><img src=/figures/hnsw_perf_4.png alt=hnsw></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf>Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></article></div></div></div></body></html>