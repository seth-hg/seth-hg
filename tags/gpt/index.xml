<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gpt on braveheat.xyz</title><link>https://www.braveheart.xyz/tags/gpt/</link><description>Recent content in gpt on braveheat.xyz</description><generator>Hugo -- gohugo.io</generator><language>zh</language><lastBuildDate>Wed, 05 Apr 2023 15:46:40 +0800</lastBuildDate><atom:link href="https://www.braveheart.xyz/tags/gpt/index.xml" rel="self" type="application/rss+xml"/><item><title>大模型时代，浅读一下LLaMA</title><link>https://www.braveheart.xyz/journals/llama-model/</link><pubDate>Wed, 05 Apr 2023 15:46:40 +0800</pubDate><guid>https://www.braveheart.xyz/journals/llama-model/</guid><description>从去年年底 ChatGPT 发布以来，大语言模型迅速升温，吸引了全世界的关注，甚至连非 IT 行业的人都知道了 ChatGPT。借着这股热潮，我也补习了一下语言模型相关的知识。这里以 Meta 开源的 LLaMA1 模型为例，通过阅读论文和代码，理解一下大语言模型的典型结构。
先了解一点背景，有助于理解 LLaMA 模型。当前主流的大语言模型，都是基于谷歌提出的 Transformer2（后文称为经典 Transformer）。Transformer 的关键贡献是引入了 attention 机制来处理上下文之间的关联性。在自然语言中，如何理解一段文本中的某个词是取决于上下文的，并且上下文中的每个词对当前词含义的影响程度也是不同的。在 Transformer 之前，这种关联性是通过 RNN 来处理的。但是 RNN 必须按顺序处理每个词，不能并行计算，attention 解决了这个问题。关于 Transformer 和 attention，这里先不展开，后面结合代码来看。
经典 Transformer 是 encoder-decoder 架构，encoder 处理输入，将其转换成状态，decoder 根据状态产生输出。形象一点的说，encoder 负责理解问题，decoder 负责做出应答。根据初代 GPT 发表的论文3，GPT 是 decoder-only 的 Transformer。LLaMA 的论文里虽然没有说，但是从代码来看，也是一样只有 decoder。至于为什么采用 decoder-only 模型，这里4有篇文章我觉得分析得很好。
相比经典 Transformer，LLaMA 做了以下三点改动：
Pre-normalization。在每一个 sub-layer 里，对输入进行 normalize，而不是对输出。使用的函数为 RMSNorm。 SwiGLU activation function。在前馈神经网络里，使用 SwiGLU 激活函数，而不是 ReLU。 Rotary Embeddings。用 rotary positional embeddings(RoPE)取代 absolute positional embeddings。 这三点改动并非 LLaMA 原创，而是来自之前的文献。想要深入了解需要去阅读原始文献。</description></item></channel></rss>